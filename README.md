# Cherryberry

Welcome to CHERRYBERRY, your friendly AI text adventure!

This game is an experiment in using LLMs for text-mode gameplay. It's able to
generate locations as you go, as well as process player actions and manage
inventory (somewhat). The gameplay is limited, so treat it as a demo.

The game is saved frequently into JSON (hence the sluggishness), but it's safe
to exit with CTRL-C anytime.  If anything goes wrong, CTRL-C, and edit the files
in the save directory. 

Happy experimenting ðŸ˜Š

## Demo

This demo has been recorded on an M1 Mac with 32 GB RAM. The demo is in real time (not sped up). You can see the model is a bit weak at times, and needs some corrections, but it's consistent enough for lightweight gameplay.

![Demo](demo.gif)

(41MB download, might take a tick âŒ›)

## Running

Download an LLM to your local machine. Recommended model is [LLaMA2-13B-Psyfighter2-GGUF](https://huggingface.co/KoboldAI/LLaMA2-13B-Psyfighter2-GGUF), the Q4_K_M variant. Why is it recommended you may ask? Because this is the model I've made the game with ðŸ˜‚

### Mac M*

*These instructions tested on Mac M1*

My preferred installation option is via micromamba. [Install micromamba](https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html), then:

```bash
micromamba create -n cherryberry -f environment.yml
micromamba activate cherryberry
```

If your system has python already installed, then you should be able to skip the above step, or you can replace it with venv installation.

Install dependencies:

```bash
CMAKE_ARGS="-DLLAMA_METAL=on" poetry install
```

If you don't like poetry, you can also install the dependencies manually using `pip` - have a look at the list in `pyproject.toml`.


```bash
python3 -m cherryberry \
	--model ../models/LLaMA2-13B-Psyfighter2.Q4_K_M.gguf \
	--n_ctx 4096 \
	--n_batch 512 \
	-ngl 1 \
	--threads 4
```

### Linux/CUDA

*These instructions tested on Ubuntu 20, with 3060 which is CUDA 8*

My preferred installation option is via micromamba. [Install micromamba](https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html).

Open up `environment-cuda.yml` and adjust cudnn and cudatoolkit-dev versions to match your GPU, then:

```bash
# This will take ages and download the Nvidia world.
micromamba create -n cherryberry -f environment-cuda.yml
micromamba activate cherryberry
```

If your system has python and CUDA dependencies already installed (for Nvidia, try to run `nvcc`), then you should be able to skip the above step, or you can replace it with venv installation.

Install dependencies:

```bash
CMAKE_ARGS="-DLLAMA_CUBLAS=on" poetry install 
```

If you don't like poetry, you can also install the dependencies manually using `pip` - have a look at the list in `pyproject.toml`.

```bash
python3 -m cherryberry \
	--model ../models/LLaMA2-13B-Psyfighter2.Q4_K_M.gguf \
	--n_ctx 4096 \
	--n_batch 512 \
	-ngl 200 \
	--threads 4
```

Change `ngl` and `threads` to suit. `ngl` should be set to the actual number of layers to offload to GPU.


### Force LlamaCpp recompilation

If you are running into problems with compiling LlamaCpp for your target, go to [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) and [LlamaCpp instructions](https://github.com/ggerganov/llama.cpp)). In particular, I'm not sure how to install this in Windows.

Warning: poetry will compile LlamaCpp only on the first install, if you change the compile time flags (`LLAMA_METAL`, `LLAMA_CUBLAS` etc) you can force recompilation with:

```bash
poetry remove llama-cpp-python
CMAKE_ARGS="-DLLAMA_CUBLAS=on" poetry add llama-cpp-python
# Note this messes around with pyproject.toml and poetry.lock
```

## Debug mode

To run the game in the debug mode:

```bash
# In one terminal window:
textual console
# In another
textual run --dev cherryberry.py --debug --model ...
```

### Ollama support

Optionally, you can use an [Ollama](https://ollama.com) API endpoint as the LLM.

First, instal Ollama by following the instructions on the website.

The model "psyfighter" doesn't exist on the Ollama hub, so you will have to create it:

Download the model from [here](https://huggingface.co/KoboldAI/LLaMA2-13B-Psyfighter2-GGUF) (the Q4_K_M variant is recommended).

Copy the provided `LLaMA2-13B-Psyfighter2.modelfile.example` file and remove the suffix.

Edit the `modelfile` to point to the downloaded model file, then run:

```bash
ollama create psyfighter -f ./LLaMA2-13B-Psyfighter2.modelfile
```

You should now be able to run the program using ollama:

```bash
python3 -m cherryberry \
	--ollama-host "http://localhost:11434" \
	--ollama-model "psyfighter"
```
